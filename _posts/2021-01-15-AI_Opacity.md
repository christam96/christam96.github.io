# Is AI opaque in a way that other technologies are not?  Should we aspire to or enforce some degree of transparency?  What are the implications of pushing on with research and development in this space if we do not or cannot resolve this? 

While a precise definition of 'opacity' in AI systems remains undefined across the readings (with Lipton arguing that any sufficient definition has failed to reach working consensus), much of the discussion surrounding opacity has focused on the production of human-interpretable explanations, or *clues*, given by AI systems as to how they arrive at their outputs. This broad definition, or lack thereof, has been scrutinized in response to calls for increased transparency and interpretability in AI systems. The motivation for these traits are warranted when considering the roles AI systems will play in our society and the ensuing consequences they will have on our livelihoods. However, teasing out interpretability in AI systems is easier said than done. Burrell's examination of the ambiguities that arise when attempting to understand the internal logic of machine learning algorithms when applied to simple tasks, such as classifying hand written digits and spam/ham emails, demonstrates the difficulties of trying to extract human meaning out of the decision making process of these algorithms. The problem of AI interpretability, and therefore of opacity, only gets more complex as the complexity of the problems we apply AI algorithms to grow. As Domingos notes, *intuition fails in high dimensions*. Is AI then opaque in a way that other technologies are not? We should recognize the degree of algorithmic transparency forfeited by deep learning models in order to operate on raw or lightly processed data. This is novel in relation to previous machine learning algorithms and so adds a dimension to AI opacity that is unaccounted for in academic literature. How might we move towards having more interpretable models? Lipton raises an interesting point about reducing the discrepancy between real-life and machine learning objectives. Can we develop richer loss functions that encode specific outcomes we'd like to achieve in the real world? Can we devise performance metrics which are able to tell us more about what we want to know? In other words, can we build more expressive models? These ideas point to lines of research that may yield progress towards more interpretable AI systems, but for now they point to very real problems in our trustworthiness of the technology. Despite a lack of consensus in the definition of opacity, it is commonly agreed that the 'black box' nature of AI systems presents challenges in the way we will operationalize these systems in society. These challenges become more pressing when considering the ubiquity in which AI systems may exist in the future. Consequences of the technology on this scale will be prolific and may draw social implications. To this end, we should aspire for some higher degree of transparency and interpretability in AI. Continuing to push on with research and development in AI without addressing the problem of opacity will severely limit the technologies adoption by the broader society. In the event where we are unable to or *choose not to* address the problem of opacity, we cannot depend on the technology in areas involving critical decision making (triage, parole decisions, medical diagnostics). In these cases, the stakes are simply too high to even have whiffs of distrust in the arbiter. Instead AI systems should be introduced as a tool for experts in these fields, used to augment their workflow and assist them in these critical decisions.  
