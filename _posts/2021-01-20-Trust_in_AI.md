# Trust in Medical AI: Substitutionism v.s. Extensionism

This commentary will focus on the debate between [Hatherley](https://pubmed.ncbi.nlm.nih.gov/32220870/) and [Ferrario at al.](https://jme.bmj.com/content/medethics/early/2020/11/25/medethics-2020-106922.full.pdf), where they discuss the topic of trust in medical AI. In *Limits of trust in medical AI*, Hatherley presents his concerns for the potential deficit of trust in clinical relationships with the displacement of epistemic authority of human doctors with the introduction of AI systems. His argument builds on classical discussions of trust where a distinction is made between reliability and trust. In these discussions, reliability is designated for those expectations which are formed descriptively or empirically. This contrasts to trust, which is preserved for those expectations evaluated on a normative basis. Normative claims are prescriptive and reflect intuitions of what we think ought to be ([Johnson & Miller, 2009](https://www.pearson.com/us/higher-education/program/Johnson-Computer-Ethics-4th-Edition/PGM48819.html)). The notion of trust extends reliability in that when we trust someone, we are relying on them to do what we wish to trust them to do, and for them to be willing to do it ([McLeod, 2020](https://plato.stanford.edu/entries/trust/)). These extended conditions of reliability define trust as requiring whomever or whatever we are trusting to have a form of agency. As Russell Hardin puts it, "there has to be a belief that one's interests are encapsulated in the interests of the trusted person and that the trusted person has the right motivations for action". If we are to accept these notions of reliability and trust, then for someone to say they trust an AI system is to merely to say that they rely on the AI system, since it is impossible for AI systems to satisfy any conditions of trust past reliability. Ferrario et al. refutes this notion by considering an account of trust that distinguishes itself from reliability in a way that is compatible with trusting non-human agents. At it's core, this account frames trust between a doctor and an agent as the point where after a period of surveillance, a doctor's beliefs in the trustworthiness of the agent no longer needs to be updated. When this time comes, the doctor may halt monitoring for the elements that make the agent trustworthy and trust is said to have been gained.

Consider that Hatherley's concern is focused on doctor-patient relationship while Ferrario et al. focuses on the patient-medical AI relationship. Their approaches are thus guided by different reasoning about outcome scenarios. Hatherley's argument is grounded in the displacement of epistemic authority of clinicians, citing more accurate and reliable clinical decisions of medical AI as grounds for removing human doctors. This relates to substitutionist versus extensionist schools of thought in medicine's 'anthropocentric predicament' in the wake of medical AI. We can see that Hatherley's discussion explicitly focuses on the substitutionist outcomes of medical AI. His arguments address the question: Since doctors can be displaced by medical AI, can patients trust those autonomous systems? On the contrary, Ferrario et al. are not so quick to accept the idea of substitutionism. By proposing an alternative account for trust, they address the question of whether doctors can trust medical AI before relinquishing clinical decision making responsibilities. For them, doctors haven't been removed from the equation yet. For me, it would seem that in order to take a substitutionists position, you need to be clear that human doctors will not survive the anthropocentric predicament of medical AI. We thus need to consider the interplay of responsibilities between doctors and medical AI before we can conclude that the roles of doctors will be erased.
